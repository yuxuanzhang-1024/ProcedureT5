{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxuan/anaconda3/envs/procedureT5/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from inference import *\n",
    "from analysis import *\n",
    "import json\n",
    "import numpy as np\n",
    "import textdistance\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_set_dict, model_path, tokenizer_path, batch_size=32, ckpt_name='', multi_gpu=True, aug_test=False):\n",
    "    model_dir = os.path.dirname(model_path)\n",
    "    test_dir_path = os.path.join(model_dir, 'test_results_'+ckpt_name)\n",
    "    os.makedirs(test_dir_path, exist_ok=True)\n",
    "    if aug_test:\n",
    "        file_name = 'augmented_test_results.jsonl'\n",
    "        test_set = test_set_dict['augmented']\n",
    "    else:\n",
    "        file_name = 'original_test_results.jsonl'\n",
    "        test_set = test_set_dict['original']\n",
    "    if os.path.exists(os.path.join(test_dir_path, file_name)):\n",
    "        print('Test results already exist. Loading from file...')\n",
    "        test_results = []\n",
    "        with open(os.path.join(test_dir_path, file_name), 'r') as f:\n",
    "            for line in f:\n",
    "                test_results.append(json.loads(line))\n",
    "        return test_results\n",
    "    else:\n",
    "        test_results = dataset_inference(test_set, model_path, tokenizer_path, batch_size=batch_size, multi_gpu=multi_gpu)\n",
    "        for i in range(len(test_results)):\n",
    "            test_results[i]['sim'] = textdistance.levenshtein.normalized_similarity(test_results[i]['target'], test_results[i]['pred'])\n",
    "        with open(os.path.join(test_dir_path, file_name), 'w') as f:\n",
    "            for item in test_results:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_printer(model_name, print_aug_test=False):\n",
    "    test_dir_list = os.listdir(f\"../../results/{model_name}\")\n",
    "    test_dir_list = [item for item in test_dir_list if 'test_results' in item]\n",
    "    test_path_list = [os.path.join(f\"../../results/{model_name}\", item) for item in test_dir_list]\n",
    "    for test_path in test_path_list:\n",
    "        print(f\"Test Path: {test_path}\")\n",
    "        results_list = []\n",
    "        if not print_aug_test:\n",
    "            with open(os.path.join(test_path, 'original_test_results.jsonl')) as f:\n",
    "                original_test_results = [json.loads(line) for line in f]\n",
    "                results_list.append(original_test_results)\n",
    "        else:\n",
    "            if os.path.exists(os.path.join(test_path, 'augmented_test_results.jsonl')):\n",
    "                with open(os.path.join(test_path, 'augmented_test_results.jsonl')) as f:\n",
    "                    augmented_test_results = [json.loads(line) for line in f]\n",
    "                    results_list.append(augmented_test_results)\n",
    "            else:\n",
    "                print(\"Augmented Test Results Not Found\")\n",
    "        for results in results_list:\n",
    "            avg_sim = round(np.mean([item['sim'] for item in results]),4)\n",
    "            acc_100 = round(len([item for item in results if item['sim'] == 1]) / len(results), 4)\n",
    "            acc_90 = round(len([item for item in results if item['sim'] >= 0.9]) / len(results), 4)\n",
    "            acc_75 = round(len([item for item in results if item['sim'] >= 0.75]) / len(results), 4)\n",
    "            acc_50 = round(len([item for item in results if item['sim'] >= 0.5]) / len(results), 4)\n",
    "            pred_list = [item['pred'] for item in results]\n",
    "            target_list = [item['target'] for item in results]\n",
    "            bleu = round(original_bleu(pred_list, target_list), 4)\n",
    "            print(f\"Model: {model_name}, Bleu: {bleu}, Ave Accuracy:{avg_sim}, Accuracy 100: {acc_100}, Accuracy 90: {acc_90}, Accuracy 75: {acc_75}, Accuracy 50: {acc_50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Results of Pistachio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results already exist. Loading from file...\n",
      "Test results already exist. Loading from file...\n"
     ]
    }
   ],
   "source": [
    "ori_test_path = '../../dataset/Pistachio_test/Original/T5/test.jsonl'\n",
    "aug_test_path = '../../dataset/Pistachio_test/Augmented/T5/test.jsonl'\n",
    "\n",
    "with open(ori_test_path) as f:\n",
    "    ori_test_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(aug_test_path) as f:\n",
    "    aug_test_data = [json.loads(line) for line in f]\n",
    "\n",
    "test_set_dict = {'original': ori_test_data, 'augmented': aug_test_data}\n",
    "\n",
    "# model_list = [\n",
    "#     ('molT5-base-aug-3-ignore-pad','epoch=8-step=200000'),\n",
    "#     ('molT5-base-ignore-pad','epoch=32-step=140000'),\n",
    "#     ('molT5-small-aug-3-ignore-pad','epoch=7-step=345000'),\n",
    "#     ('molT5-small-ignore-pad','epoch=31-step=265000'),\n",
    "#     ('text-chemT5-base-aug-3-ignore-pad', 'epoch=8-step=190000'),\n",
    "#     ('text-chemT5-base-ignore-pad', 'epoch=25-step=110000'),\n",
    "#     ('text-chemT5-small-aug-3-ignore-pad', 'epoch=9-step=450000'),\n",
    "#     ('text-chemT5-small-ignore-pad', 'epoch=43-step=370000'),\n",
    "#     ('T5-base-aug-3-ignore-pad','epoch=7-step=185000'),\n",
    "#     ('T5-base-ignore-pad','epoch=24-step=105000')]\n",
    "\n",
    "model_list = [\n",
    "    ('molT5-base-aug-3-ignore-pad','epoch=8-step=200000'),\n",
    "    ('molT5-base-ignore-pad','epoch=32-step=140000')]\n",
    "for model_name, ckpt_name in model_list:\n",
    "    model_path = f\"../../results/{model_name}/hf_model\"\n",
    "    tokenizer_path = f\"../../results/{model_name}/hf_model\"\n",
    "    if 'small' in model_name:\n",
    "        batch_size = 64\n",
    "    else:\n",
    "        batch_size = 32\n",
    "    test_results = evaluation(test_set_dict, model_path, tokenizer_path, batch_size=batch_size, ckpt_name=ckpt_name, multi_gpu=True, aug_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Path: ../../results/molT5-base-aug-3-ignore-pad/test_results_epoch=8-step=200000\n",
      "Model: molT5-base-aug-3-ignore-pad, Bleu: 0.6032, Ave Accuracy:0.6387, Accuracy 100: 0.0548, Accuracy 90: 0.1283, Accuracy 75: 0.2798, Accuracy 50: 0.7372\n",
      "Test Path: ../../results/molT5-base-ignore-pad/test_results_epoch=32-step=140000\n",
      "Model: molT5-base-ignore-pad, Bleu: 0.5509, Ave Accuracy:0.5986, Accuracy 100: 0.021, Accuracy 90: 0.0669, Accuracy 75: 0.1953, Accuracy 50: 0.6817\n"
     ]
    }
   ],
   "source": [
    "for model_name, _ in model_list:\n",
    "    metric_printer(model_name, print_aug_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pred-test-1000000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67638/67638 [16:45<00:00, 67.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu: 0.559, Accuracy 100: 0.0396, Accuracy 90: 0.1105, Accuracy 75: 0.2596, Accuracy 50: 0.7012\n",
      "Processing pred-test-910000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67638/67638 [16:53<00:00, 66.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu: 0.5598, Accuracy 100: 0.0401, Accuracy 90: 0.109, Accuracy 75: 0.2596, Accuracy 50: 0.6976\n",
      "Processing pred-test-920000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67638/67638 [16:38<00:00, 67.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu: 0.5579, Accuracy 100: 0.0396, Accuracy 90: 0.1087, Accuracy 75: 0.2587, Accuracy 50: 0.6985\n",
      "Processing pred-test-930000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67638/67638 [16:37<00:00, 67.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu: 0.5563, Accuracy 100: 0.04, Accuracy 90: 0.1089, Accuracy 75: 0.2576, Accuracy 50: 0.6982\n",
      "Processing pred-test-940000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67638/67638 [16:33<00:00, 68.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu: 0.557, Accuracy 100: 0.0406, Accuracy 90: 0.1104, Accuracy 75: 0.2606, Accuracy 50: 0.6932\n",
      "Processing pred-test-950000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67638/67638 [16:38<00:00, 67.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu: 0.5574, Accuracy 100: 0.0399, Accuracy 90: 0.1096, Accuracy 75: 0.258, Accuracy 50: 0.698\n",
      "Processing pred-test-960000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67638/67638 [16:36<00:00, 67.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu: 0.558, Accuracy 100: 0.04, Accuracy 90: 0.1092, Accuracy 75: 0.2605, Accuracy 50: 0.699\n",
      "Processing pred-test-970000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67638/67638 [16:43<00:00, 67.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu: 0.5594, Accuracy 100: 0.0405, Accuracy 90: 0.1105, Accuracy 75: 0.2604, Accuracy 50: 0.6993\n",
      "Processing pred-test-980000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67638/67638 [16:59<00:00, 66.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu: 0.5611, Accuracy 100: 0.0406, Accuracy 90: 0.1101, Accuracy 75: 0.2607, Accuracy 50: 0.7025\n",
      "Processing pred-test-990000.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67638/67638 [16:50<00:00, 66.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu: 0.5615, Accuracy 100: 0.0406, Accuracy 90: 0.1097, Accuracy 75: 0.2582, Accuracy 50: 0.7027\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import textdistance\n",
    "transformer_results_path = '../../dataset/Pistachio_Aug_1/transformer'\n",
    "test_data = os.path.join(transformer_results_path, 'tgt-test.txt')\n",
    "with open(test_data, 'r') as f:\n",
    "    test_data = [line.strip() for line in f]\n",
    "pred_results_list = os.listdir(os.path.join(transformer_results_path, 'results_txt'))\n",
    "os.makedirs(os.path.join(transformer_results_path, 'results_jsonl'), exist_ok=True)\n",
    "for pred_file_path in pred_results_list:\n",
    "    print(f\"Processing {pred_file_path}\")\n",
    "    pred_file_path = os.path.join(transformer_results_path, 'results_txt', pred_file_path)\n",
    "    with open(pred_file_path, 'r') as f:\n",
    "        pred_data = [line.strip() for line in f]\n",
    "    result_path = os.path.join(transformer_results_path, 'results_jsonl', os.path.basename(pred_file_path).replace('.txt', '.jsonl'))\n",
    "    result_list = []\n",
    "    with open(result_path, 'w') as f:\n",
    "        for i in tqdm(range(len(test_data))):\n",
    "            sim = textdistance.levenshtein.normalized_similarity(test_data[i], pred_data[i])\n",
    "            f.write(json.dumps({'target': test_data[i], 'pred': pred_data[i], 'sim': sim}) + '\\n')\n",
    "            result_list.append({'target': test_data[i], 'pred': pred_data[i], 'sim': sim})\n",
    "    acc_100 = round(len([item for item in result_list if item['sim'] == 1]) / len(result_list), 4)\n",
    "    acc_90 = round(len([item for item in result_list if item['sim'] >= 0.9]) / len(result_list), 4)\n",
    "    acc_75 = round(len([item for item in result_list if item['sim'] >= 0.75]) / len(result_list), 4)\n",
    "    acc_50 = round(len([item for item in result_list if item['sim'] >= 0.5]) / len(result_list), 4)\n",
    "    bleu = round(original_bleu(pred_data, test_data), 4)\n",
    "    print(f\"Bleu: {bleu}, Accuracy 100: {acc_100}, Accuracy 90: {acc_90}, Accuracy 75: {acc_75}, Accuracy 50: {acc_50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67499/67499 [13:27<00:00, 83.60it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "transformer_results_path = '../../yuxuan/smiles2actions/Augmented'\n",
    "test_data = os.path.join(transformer_results_path, 'tgt-test.txt')\n",
    "pred_data = os.path.join(transformer_results_path, 'pred-test.txt')\n",
    "result_path = os.path.join(transformer_results_path, 'result.jsonl')\n",
    "\n",
    "with open(test_data, 'r') as f:\n",
    "    test_data = [line.strip() for line in f]\n",
    "\n",
    "with open(pred_data, 'r') as f:\n",
    "    pred_data = [line.strip() for line in f]\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(result_path, 'w') as f:\n",
    "    for i in tqdm(range(len(test_data))):\n",
    "        sim = textdistance.levenshtein.normalized_similarity(test_data[i], pred_data[i])\n",
    "        f.write(json.dumps({'target': test_data[i], 'pred': pred_data[i], 'sim': sim}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67499/67499 [13:43<00:00, 81.95it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "transformer_results_path = '../../yuxuan/smiles2actions/transformer'\n",
    "test_data = os.path.join(transformer_results_path, 'tgt-test.txt')\n",
    "pred_data = os.path.join(transformer_results_path, 'pred_256.txt')\n",
    "result_path = os.path.join(transformer_results_path, 'result_256.jsonl')\n",
    "\n",
    "with open(test_data, 'r') as f:\n",
    "    test_data = [line.strip() for line in f]\n",
    "\n",
    "with open(pred_data, 'r') as f:\n",
    "    pred_data = [line.strip() for line in f]\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(result_path, 'w') as f:\n",
    "    for i in tqdm(range(len(test_data))):\n",
    "        sim = textdistance.levenshtein.normalized_similarity(test_data[i], pred_data[i])\n",
    "        f.write(json.dumps({'target': test_data[i], 'pred': pred_data[i], 'sim': sim}) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: transformer_aug_1, Bleu: 0.554, Accuracy 100: 0.0339, Accuracy 90: 0.1029, Accuracy 75: 0.2561, Accuracy 50: 0.6962\n"
     ]
    }
   ],
   "source": [
    "with open(result_path, 'r') as f:\n",
    "    results = [json.loads(line) for line in f]\n",
    "model_name = 'transformer_aug_1'\n",
    "acc_100 = round(len([item for item in results if item['sim'] == 1]) / len(results), 4)\n",
    "acc_90 = round(len([item for item in results if item['sim'] >= 0.9]) / len(results), 4)\n",
    "acc_75 = round(len([item for item in results if item['sim'] >= 0.75]) / len(results), 4)\n",
    "acc_50 = round(len([item for item in results if item['sim'] >= 0.5]) / len(results), 4)\n",
    "pred_list = [item['pred'] for item in results]\n",
    "target_list = [item['target'] for item in results]\n",
    "bleu = round(original_bleu(pred_list, target_list), 4)\n",
    "print(f\"Model: {model_name}, Bleu: {bleu}, Accuracy 100: {acc_100}, Accuracy 90: {acc_90}, Accuracy 75: {acc_75}, Accuracy 50: {acc_50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Results of Orgsyn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../dataset/Orgsyn_HC/test.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m ori_test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../dataset/Orgsyn_HC/test.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mori_test_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     ori_test_data \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[1;32m      6\u001b[0m test_set_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m: ori_test_data}\n",
      "File \u001b[0;32m~/anaconda3/envs/procedureT5/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../dataset/Orgsyn_HC/test.jsonl'"
     ]
    }
   ],
   "source": [
    "ori_test_path = '../../dataset/Orgsyn_HC/test.jsonl'\n",
    "\n",
    "with open(ori_test_path) as f:\n",
    "    ori_test_data = [json.loads(line) for line in f]\n",
    "\n",
    "test_set_dict = {'original': ori_test_data}\n",
    "model_list = [\n",
    "    ('molT5-base-orgsyn-hc-aug'),\n",
    "    ('molT5-base-orgsyn-hc-mix-1'),\n",
    "    ('molT5-base-orgsyn-hc-mix-2'),\n",
    "    ('molT5-base-orgsyn-hc-mix-3'),\n",
    "    ('molT5-base-orgsyn-hc-mix-random')\n",
    "]\n",
    "for model_name in model_list:\n",
    "    ckpt_list = os.listdir(f\"../../results/{model_name}\")\n",
    "    ckpt_list = [item.split('.')[0] for item in ckpt_list if 'ckpt' in item]\n",
    "    for i, ckpt_name in enumerate(ckpt_list):\n",
    "        model_path = f\"../../results/{model_name}/hf_model_{i+1}\"\n",
    "        tokenizer_path = f\"../../results/{model_name}/hf_model_{i+1}\"\n",
    "        if 'small' in model_name:\n",
    "            batch_size = 64\n",
    "        else:\n",
    "            batch_size = 32\n",
    "        test_results = evaluation(test_set_dict, model_path, tokenizer_path, batch_size=batch_size, ckpt_name=ckpt_name, multi_gpu=True, aug_test=False)\n",
    "    # test_results = evaluation(test_set_dict, model_path, tokenizer_path, batch_size=batch_size, ckpt_name=ckpt_name, multi_gpu=True, aug_test=True)\n",
    "for model_name in model_list:\n",
    "    metric_printer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Path: ../../results/molT5-base-orgsyn-mix-rag-1/test_results_epoch=37-step=34000\n",
      "Model: molT5-base-orgsyn-mix-rag-1, Bleu: 0.3879, Ave Accuracy:0.4953, Accuracy 100: 0.0, Accuracy 90: 0.0, Accuracy 75: 0.0483, Accuracy 50: 0.4207\n",
      "Test Path: ../../results/molT5-base-orgsyn-mix-random/test_results_epoch=49-step=41500\n",
      "Model: molT5-base-orgsyn-mix-random, Bleu: 0.3949, Ave Accuracy:0.4883, Accuracy 100: 0.0, Accuracy 90: 0.0, Accuracy 75: 0.0207, Accuracy 50: 0.4276\n"
     ]
    }
   ],
   "source": [
    "model_list = [\n",
    "('molT5-base-orgsyn-mix-rag-1', 'epoch=37-step=34000'),\n",
    "('molT5-base-orgsyn-mix-random', 'epoch=49-step=41500')]\n",
    "for model_name, _ in model_list:\n",
    "    metric_printer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
